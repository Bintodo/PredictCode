{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scipy vs our code\n",
    "\n",
    "The `scipy` Gaussian kernel density estimation method is [documented here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html#scipy.stats.gaussian_kde).  From reading the source code, and comparing with [\"Density Estimation for Statistics and Data Analysis\" by Silverman](https://books.google.co.uk/books?id=e-xsrjsL7WkC&lpg=PR9&ots=iwStts0DZr&dq=density%20estimation%20for%20statistics%20and%20data%20analysis&lr&pg=PP1#v=onepage&q=density%20estimation%20for%20statistics%20and%20data%20analysis&f=false) see equation 4.7 for example, the method is as follows.\n",
    "\n",
    "- Input is $(X_i)_{i=1}^n$ points in a $d$-dimensional space, say each $X_i = (X^{(i)}_j)_{j=1}^d$\n",
    "- Compute the covariance matrix from this sample:\n",
    "$$ S_{j,k} = \\frac{1}{n-1} \\sum_i (X^{(i)}_j - \\overline{X}_j)(X^{(i)}_k - \\overline{X}_k) $$\n",
    "where $\\overline{X}_j = \\frac{1}{n} \\sum_i X^{(i)}_j$ is the sample mean of the $j$th coordinate.\n",
    "- Letting $S^{-1}$ be the inverse matrix and $|S|$ the determinate of $S$, choose $h$, a \"bandwidth\", and $K$, a \"symmetric kernel\".  Then our density estimation is\n",
    "$$ f(x) = \\frac{1}{|S|^{1/2} h^d n} \\sum_{i=1}^n K\\big( h^{-2}  (X_i-x)^t S^{-1} (X_i-x) \\big) $$\n",
    "- For example, the Gaussian kernel gives $K(t) = (2\\pi)^{-d/2} \\exp(-t/2)$\n",
    "- As stated by Silverman, this procedure is equivalent to applying a linear transformation to the input data so that it has unit covariance, forming the standard KDE, and then transforming back.\n",
    "- The bandwidth can be chosen by eye, or we can use a \"rule of thumb\".  Here we follow `scipy` and use Scott's rule, which is $h = n^{-1/(4+d)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GaussianKernel():\n",
    "    \"\"\"Expect input as array of shape `(d,N)` of `N` samples in `d`-dimensional space.\"\"\"\n",
    "    def __init__(self, data, bandwidth=None):\n",
    "        self.data = np.asarray(data)\n",
    "        if len(self.data.shape) == 0:\n",
    "            raise ValueError(\"Cannot be a scalar\")\n",
    "        if len(self.data.shape) == 1:\n",
    "            self.data = self.data[None,:]\n",
    "        \n",
    "        self.space_dims, self.num_data_points = self.data.shape\n",
    "        if bandwidth is None:\n",
    "            bandwidth = self._scott()\n",
    "        \n",
    "        self.covariance = np.atleast_2d(np.cov(data, rowvar=1, bias=False))\n",
    "        self.inv_cov = scipy.linalg.inv(self.covariance) / (bandwidth**2)\n",
    "        cdet = np.sqrt(scipy.linalg.det(self.covariance))\n",
    "        \n",
    "        self.normalisation = cdet * (2 * np.pi * bandwidth * bandwidth) ** (self.space_dims/2)\n",
    "        self.normalisation *= self.num_data_points\n",
    "        \n",
    "    def _scott(self):\n",
    "        return self.num_data_points ** (-1 / (4 + self.space_dims))\n",
    "        \n",
    "    def __call__(self, t):\n",
    "        t = np.asarray(t)\n",
    "        if len(t.shape) == 0:\n",
    "            if self.space_dims != 1:\n",
    "                raise ValueError(\"Expect {} dimensional input\".format(space_dims))\n",
    "            t = t[None]\n",
    "        if len(t.shape) == 1:\n",
    "            t = t[None,:]\n",
    "\n",
    "        x = self.data[:,:,None] - t[:,None,:]\n",
    "        x = np.sum(x * np.sum(self.inv_cov[:,:,None,None] * x[:,None,:,:], axis=0), axis=0)\n",
    "\n",
    "        return np.sum(np.exp(-x / 2), axis=0) / self.normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.random.random(size=(2,20))\n",
    "k = GaussianKernel(data)\n",
    "kernel = scipy.stats.kde.gaussian_kde(data, bw_method=\"scott\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.random(size=(2,100))\n",
    "np.testing.assert_allclose(kernel(x), k(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.random.random(size=20)\n",
    "k = GaussianKernel(data)\n",
    "kernel = scipy.stats.kde.gaussian_kde(data, bw_method=\"scott\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.random(size=100)\n",
    "np.testing.assert_allclose(kernel(x), k(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.random.random(size=(3,20))\n",
    "k = GaussianKernel(data)\n",
    "kernel = scipy.stats.kde.gaussian_kde(data, bw_method=\"scott\")\n",
    "\n",
    "x = np.random.random(size=(3,100))\n",
    "np.testing.assert_allclose(kernel(x), k(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stocastic declusting\n",
    "\n",
    "To convert the \"stocastic declusting\" algorithm into a more \"EM-like\" algorithm, I would like to do the following:\n",
    "\n",
    "- Start with data $(X_i)_{i=1}^m$ and a probability distribution on $\\{1,2,\\cdots,m\\}$.\n",
    "- Using the probability distribution, sample from the data, giving $(Y_i)_{i=1}^n$ say.\n",
    "- Form the density estimate $f$ using $(Y_i)$.\n",
    "- Thus $f$ is a random variable, and it makes sense to ask what the expectation of $f$ is (with respect to our probability distribution on $\\{1,2,\\cdots,m\\}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
