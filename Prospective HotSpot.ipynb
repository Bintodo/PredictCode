{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. Bowers, Johnson, Pease, \"Prospective hot-spotting: The future of crime mapping?\", Brit. J. Criminol. (2004) 44 641--658.  doi:10.1093/bjc/azh036\n",
    "\n",
    "2. Johnson et al., \"Prospective crime mapping in operational context\", Home Office Online Report 19/07  [Police online library](http://library.college.police.uk/docs/hordsolr/rdsolr1907.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "### Grid the space\n",
    "\n",
    "Divide the area of interest into a grid.  The grid is used for both the algorithm, and for data visualisation.  There is some discussion about reasonable grid sizes.  10m by 10m or 50m by 50m have been used.\n",
    "\n",
    "### Aim of algorithm\n",
    "\n",
    "We select \"bandwidths\": space/time regions of the data.  Common values are the look at events within 400m and the last 2 months (8 weeks).  For each grid cell, for each event falling in this range, we compute a weighting for the event, and then sum all the weightings to produce a (un-normalised) \"risk intensity\" for that cell.\n",
    "\n",
    "### Choice of weights\n",
    "\n",
    "I believe the original paper (1) is unclear on this.  The discussion on page 9 shows a formula involving \"complete 1/2 grid widths\" but does not give details as to how, _exactly_, such a distance is to be computed.  The next paragraph gives a couple of examples which seem unclear, as it simply talks about \"neighbouring cell\".  No formula is given for total weight, but we can infer it from the examples.\n",
    "\n",
    "Let $t_i$ be the elapsed time between now and the event, and $d_i$ the distance of event $i$ from the centre of the grid cell we are interested in.  Then\n",
    "$$ w = \\sum_{t_i \\leq t_\\max, d_i \\leq d_\\max} \\frac{1}{1+d_i} \\frac{1}{1+t_i}. $$\n",
    "For this to make sense, we introduce units:\n",
    "\n",
    "   - $t_i$ is the number of whole weeks which have elapsed.  So if today is 20th March, and the event occurred on the 17th March, $t_i=0$.  If the event occurred on the 10th, $t_i=1$.\n",
    "   - $d_i$ is the number of \"whole 1/2 grid widths between the event\" and the centre of the cell.  Again, this is slightly unclear, as an event occurring very near the edge of a cell would (thanks to Pythagoras) have $d_i=1$, while the example in the paper suggests always $d_i=0$ in this case.  We shall follow the precise definition.\n",
    "   \n",
    "Paper (2) uses a different formula, and gives no examples:\n",
    "\n",
    "$$ w = \\sum_{t_i \\leq t_\\max, d_i \\leq d_\\max} \\Big( 1 + \\frac{1}{d_i} \\Big) \\frac{1}{t_i}. $$\n",
    "\n",
    "where we told only that:\n",
    "\n",
    "   - $t_i$ is the elapsed time (but using $1/t_i$ suggests very large weights for events occurring very close to the time of analysis).\n",
    "   - $d_i$ is the \"number of cells\" between the event and the cell in question.  The text also notes: \"Thus, if a crime occurred within the cell under consideration, the distance would be zero (actually, for computational reasons 1) if it\n",
    "occurred within an adjacent cell, two, and so on.\"  What to do about diagonal cells is not specified: sensible choices might be that a cell diagonally offset from the cell of interest is either distance 2 or 3.  However, either choice would seem to introduce an anisotropic component which seems unjustified.\n",
    "\n",
    "It is not clear to me that (2) gives the temporal and spatial bandwidths used.\n",
    "\n",
    "### Coupled units\n",
    "\n",
    "Notice that both weight functions couple the \"units\" of time and space.  For example, if we halve the cell width used, then (roughly speaking) each $d_i$ will double, while the $t_i$ remain unchanged.  This results in the time component now having a larger influence on the weight.\n",
    "\n",
    "   - It hence seems sensible that we scale both time and distance together.\n",
    "   - If we run one test with a grid size of 50m and the time unit as 7 days,\n",
    "   - then another test could be a grid size of 25m, but also with the time unit as 3.5 days.\n",
    "\n",
    "### Variations\n",
    "\n",
    "Paper (2) introduces a variation of this method:\n",
    "\n",
    "> For the second set of results for the prospective method, for each cell, the crime that confers\n",
    "the most risk is identified and the cell is assigned the risk intensity value generated by that\n",
    "one point.\n",
    "\n",
    "Again, this is not made _entirely_ clear, but I believe it means that we look at the sum above, and instead of actually computing the sum, we compute each _summand_ and then take the largest value to be the weight.\n",
    "\n",
    "### Generating predictions\n",
    "\n",
    "The \"risk intensity\" for each grid cell is computed, and then displayed graphically as relative risk.  For example:\n",
    "\n",
    "   - Visualise by plotting the top 1% of grid cells, top 5% and top 10% as different colours.  Paper (2) does this.\n",
    "   - Visualise by generating a \"heat map\".  Paper (1) does this.\n",
    "   \n",
    "When using the risk intensity to make predictions, there are two reasonable choices:\n",
    "\n",
    "1. Compute the risk intensity for today, using all the data up until today.  Treat this as a risk profile for the next few days in time.\n",
    "2. For each day into the future we wish to predict, recompute the risk intensity.\n",
    "\n",
    "The difference between (1) and (2) is that events may change their time-based weighting (or event fall out of the temporal bandwidth completely).  For example, if today is the 20th March and an event occurred on the 14th, we consider it as occuring zero whole weeks ago, and so it contributes a weight of $1/1 = 1$ (in the 1st formula, for example).  However, if we recompute the risk for the 22nd March, this event is now one whole week in the past, and so the weight becomes $1/2$.\n",
    "\n",
    "### Aliasing issues\n",
    "\n",
    "This issue falls under what I term an \"aliasing issue\" which comes about as we are taking continuous data and making it discrete:\n",
    "\n",
    "   - We lay down a grid, making space discrete, because we measure distance as some multiple of \"whole grid width\".\n",
    "   - We measure time in terms of \"whole weeks\" but seem to make day level predictions.\n",
    "   \n",
    "It would appear, a priori, that changing the offset of the grid (e.g. moving the whole grid 10m north) could cause a lot of events to jump from one grid cell to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "We keep the grid for \"prediction\" purposes, but we allow a large range of \"weights\" to be plugged in, from various \"guesses\" as to what the exactly the original studies used, to variations of our own making.\n",
    "\n",
    "Note, however, that this is still ultimately a \"discrete\" algorithm.  We give a variation which generates a continuous kernel (and then bins the result for visualisation / comparision purposes) as a different prediction method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import open_cp\n",
    "import open_cp.prohotspot as phs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 68.99449305, -18.68248891],\n",
       "       [ 91.10647407, -19.90010428],\n",
       "       [ 85.30882659, -18.13107613],\n",
       "       [ 84.96479956, -17.97288045],\n",
       "       [ 68.88482081, -17.67102775],\n",
       "       [ 77.51379752, -15.14940798],\n",
       "       [ 89.90020311, -14.33244934],\n",
       "       [ 53.64697538, -13.6744408 ],\n",
       "       [ 83.3870943 , -13.76446312],\n",
       "       [ 86.7713431 , -18.04446361],\n",
       "       [ 86.92105554, -18.12153375],\n",
       "       [ 97.79933395, -18.79188021],\n",
       "       [ 95.67031429, -11.19746285],\n",
       "       [ 71.6308542 , -16.65951133],\n",
       "       [ 83.93597405, -13.18928524],\n",
       "       [ 76.63783453, -14.4761322 ],\n",
       "       [ 93.72336824, -17.41285243],\n",
       "       [ 68.55555927, -16.73044268],\n",
       "       [ 69.97297061, -17.26866978],\n",
       "       [ 58.37996541, -13.96590989]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "times = [datetime.datetime(2017,3,10) + datetime.timedelta(days=np.random.randint(0,10)) for _ in range(20)]\n",
    "times.sort()\n",
    "xc = np.random.random(size=20) * 50 + 50\n",
    "yc = np.random.random(size=20) * 10 - 20\n",
    "points = open_cp.TimedPoints.from_coords(times, xc, yc)\n",
    "points.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 68.99449305 -18.68248891]\n",
      "[ 91.10647407 -19.90010428]\n",
      "[ 85.30882659 -18.13107613]\n",
      "[ 84.96479956 -17.97288045]\n",
      "[ 68.88482081 -17.67102775]\n",
      "[ 77.51379752 -15.14940798]\n",
      "[ 89.90020311 -14.33244934]\n",
      "[ 53.64697538 -13.6744408 ]\n",
      "[ 83.3870943  -13.76446312]\n",
      "[ 86.7713431  -18.04446361]\n",
      "[ 86.92105554 -18.12153375]\n",
      "[ 97.79933395 -18.79188021]\n",
      "[ 95.67031429 -11.19746285]\n",
      "[ 71.6308542  -16.65951133]\n",
      "[ 83.93597405 -13.18928524]\n",
      "[ 76.63783453 -14.4761322 ]\n",
      "[ 93.72336824 -17.41285243]\n",
      "[ 68.55555927 -16.73044268]\n",
      "[ 69.97297061 -17.26866978]\n",
      "[ 58.37996541 -13.96590989]\n"
     ]
    }
   ],
   "source": [
    "for p in points.coords:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-18.68248891, -19.90010428, -18.13107613, -17.97288045,\n",
       "       -17.67102775, -15.14940798, -14.33244934, -13.6744408 ,\n",
       "       -13.76446312, -18.04446361, -18.12153375, -18.79188021,\n",
       "       -11.19746285, -16.65951133, -13.18928524, -14.4761322 ,\n",
       "       -17.41285243, -16.73044268, -17.26866978, -13.96590989])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.coords[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
