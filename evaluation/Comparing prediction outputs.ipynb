{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing prediction outputs\n",
    "\n",
    "In this notebook, we introduce the idea of comparing the outputs of predictions to what really happened: that is, \"scoring\" predictions.\n",
    "\n",
    "- At this stage, we shall not look at hot-spot production from a prediction, nor ways to compare these hotspots.\n",
    "\n",
    "\n",
    "## What are \"predictions\"?\n",
    "\n",
    "We have implemented a variety of prediction algorithms.  Some, for example the SEPP methods, have an explicit statistical model of a random process, attempt to fit data to that process, and then issue a prediction based on that fitting.  Others, for example the classic Retrospective or Prospective hot-spotting techniques, produce a \"ranking\" of different grid cells, but make no attempt to give meaning to the actual values of \"risk\" produced.\n",
    "\n",
    "A prediction might also be on a network, but except for topological issues, we can think of this as simply a prediction with \"grid\" replaced by \"edge of a network\".\n",
    "\n",
    "In principle, many prediction methods produce a (at least piecewise) continuous function which is then integrated to produce a grid prediction.  At present, we have no way to use a continuous prediction, and so we shall not develop a way to compare them.\n",
    "\n",
    "**Summary:**\n",
    "- A prediction should assign a relative risk to each grid cell, in such a way that when the cells are ordered, they are ordered from most risky to least risky.\n",
    "- A prediction may assign a genuine probability distribution to the grid, but we should not assume this.\n",
    "\n",
    "\n",
    "## What is \"scoring\"?\n",
    "\n",
    "Simply put, some way of comparing a prediction to reality.\n",
    "\n",
    "None of the prediction methods make any claim to predict that \"a crime will definitely occur here\" or that \"a crime will not occur here\".  It is hence best to think of these predictions as [probabilistic forecasts](https://en.wikipedia.org/wiki/Probabilistic_forecasting) (with the caveat above that some predicitons cannot be expect to produce true probability distributions).\n",
    "\n",
    "Probabilistic forecasting of spatial data is considered in the field of Meteorology, see for example http://www.cawcr.gov.au/projects/verification/  As this site say.\n",
    "\n",
    "> In general, it is difficult to verify a single probabilistic forecast.\n",
    "\n",
    "Instead, we compare a series of predictions against the series of actual events we are trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The techniques\n",
    "\n",
    "In other notebooks we will study in detail:\n",
    "\n",
    "### Hit rate with coverage\n",
    "\n",
    "Pick a coverage level, say 10%, select the top 10% of grid cells flagged, and then calculate the percentage of actual events captured by this 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
